{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGTDXelVj3OE"
   },
   "source": [
    "\n",
    "# TMA4215 Project 2\n",
    "## Group 11 \n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxmkzRAjp3w0"
   },
   "source": [
    "The motivation for this assignment is to understand the principals behind deep learning, and we are going to solve a simplified test case. We simply look at points in the plane, and attach them a color, either red or blue. The purpose is to try to understand if there is a correlation between the color and the coordinates of the point. Our input data is therefore a large set of points with coordinates $(x_i, y_i)$,$ i = 1, \\dots, N$, and each given a color $c_i$. Since we want our points to be embedded in $\\mathbb{R}^4$, we add two variables $z_i$, $w_i$. Define a matrix $Y_0 \\in \\mathbb{R}^{Nx4}$ as follows \n",
    "\n",
    "\\begin{bmatrix}\n",
    "    x_{1}       & y_{1} & z_{1} &  w_{1} \\\\\n",
    "    x_{2}       & y_{2} & z_{2} &  w_{2} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "    x_{N}       & y_{N} & z_{N} & w_{N}\n",
    "\\end{bmatrix}.\n",
    "\n",
    "We assume that the augmented data $Y_0$ is transformed by a differentiable equation\n",
    "\n",
    "\\begin{equation}\n",
    "Y' = \\sigma (Y \\cdot K(t)), Y(0) = Y_0,\n",
    "\\end{equation}\n",
    "\n",
    "where $K(t)$ is a 4x4 matrix for every t, and our parameters for the model. We define $\\sigma (x) = \\tanh(x)$. To get $Y_M$, we apply Eulers method on $Y_0$ with M steps. \n",
    "\n",
    "To read off a color of each point, we need something to project $Y_M$ onto, and we introduce a projection vector $W \\in \\mathbb{R}^4$. Then the projection becomes \n",
    "\n",
    "\\begin{equation}\n",
    "\\eta (Y_M \\cdot W),\n",
    "\\end{equation}\n",
    "where $\\eta(x) = e^x / (e^x + 1)$. These projected values will either be close to 0, or close to 1. The values close to 0 we judge to be blue, and red otherwise. In our validate-function below, these are the values we compare to the $c_i$-values, and evaluate how many times the color matches. \n",
    "\n",
    "In order to optimize our algorithm, we define an objective function\n",
    "\n",
    "\\begin{equation}\n",
    "J_0(Y_M, W) = \\frac{1}{2} \\| \\eta(Y_M \\cdot W) - C \\| ^2 _2,\n",
    "\\end{equation}\n",
    "\n",
    "that we wish to minimize, by iteratively calculating the gradient of our objective function numerically, and updating our values for K and W based on these calculations. \n",
    "\n",
    "Our code is presented and commented below. \n",
    "\n",
    "More training data, N, a higher number of steps , M, and a smaller stepsize, h, are factors that will give a more consistent result. These variables were chosen as a compromise between runtime and getting a satisfying result, meaning that choices that would have increased the accuracy also would have caused an increase in the runtime. The KÂ´s were set to be identity matrices and W to a 4-dimensional vector of ones. The numerical differentitation increment, epsilon, and the gradient decent parameter, tau, were chosen to be $10^{-5}$ and $0.5$, respectively. Similarily, these parameters were chosen based on written hints in the project description and verbal hints from the lecturer and teaching assistants, as well as running time and accuracy considerations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KQknMH7enin"
   },
   "source": [
    "### Python implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MgVfxSsSc3aZ",
    "outputId": "8c8aeb0c-db16-4cc5-df67-ae0a099925c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J after  5000  iterations:  0.03537109140578727\n",
      "Percentage of cases where points are classified correctly:  96.0 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as nl\n",
    "import numpy.random as rnd\n",
    "\n",
    "\n",
    "#################   Make circle problem   ##################\n",
    " \n",
    "def make_circle_problem(n, nx, PLOT):\n",
    "    # This python-script uses the following three input parameters:\n",
    "    #   n       - Number of points.\n",
    "    #   nx      - Resolution of the plotting.\n",
    "    #   PLOT    - Boolean variable for plotting.\n",
    "\n",
    "    # Defining function handles.\n",
    "    transform_domain = lambda r : 2*r-1\n",
    "    rad = lambda x1,x2 : np.sqrt(x1**2+x2**2)\n",
    "\n",
    "    # Initializing essential parameters.\n",
    "    r = np.linspace(0,1,nx)\n",
    "    x = transform_domain(r)\n",
    "    dx = 2/nx\n",
    "    x1,x2 = np.meshgrid(x,x)\n",
    "\n",
    "    # Creating the data structure 'problem' in terms of dictionaries.\n",
    "    problem = {'domain':{'x1':x,'x2':x},'classes':[None,None]}\n",
    "    group1 = {'mean_rad':0,'sigma':0.1,'prob_unscaled':lambda x1,x2: 0,'prob':lambda x1,x2: 0,'density':0}\n",
    "    group1['prob_unscaled'] = lambda x,y : np.exp(-(rad(x,y)-group1['mean_rad'])**2/(2*group1['sigma']**2))\n",
    "    density_group1 = group1['prob_unscaled'](x1,x2)\n",
    "    int_density_group1 = (dx**2)*sum(sum(density_group1))\n",
    "    group1['density'] = density_group1/int_density_group1\n",
    "    group2 = {'mean_rad':0.5,'sigma':0.1,'prob_unscaled':lambda x1,x2: 0,'prob':lambda x1,x2: 0,'density':0}\n",
    "    group2['prob_unscaled'] = lambda x,y : np.exp(-(rad(x,y)-group2['mean_rad'])**2/(2*group2['sigma']**2))\n",
    "    density_group2 = group2['prob_unscaled'](x1,x2)\n",
    "    int_density_group2 = (dx**2)*sum(sum(density_group2))\n",
    "    group2['density'] = density_group2/int_density_group2\n",
    "    problem['classes'][0] = group1\n",
    "    problem['classes'][1] = group2\n",
    "\n",
    "    # Creating the arrays x1 and x2.\n",
    "    x1 = np.zeros((n,2))\n",
    "    x2 = np.zeros((n,2))\n",
    "    count = 0\n",
    "    for i in range(0,n):\n",
    "        count += 1\n",
    "        N1 = 'x1_'+str(count)+'.png'\n",
    "        N2 = 'x2_'+str(count)+'.png'\n",
    "        x1[i,0],x1[i,1] = pinky(problem['domain']['x1'],problem['domain']['x2'],problem['classes'][0]['density'],PLOT,N1)\n",
    "        x2[i,0],x2[i,1] = pinky(problem['domain']['x1'],problem['domain']['x2'],problem['classes'][1]['density'],PLOT,N2)\n",
    "\n",
    "    # Creating the data structure 'data' in terms of dictionaries.\n",
    "    x = np.concatenate((x1[0:n,:],x2[0:n,:]),axis=0)\n",
    "    y = np.concatenate((np.ones((n,1)),2*np.ones((n,1))),axis=0)\n",
    "    i = rnd.permutation(2*n)\n",
    "    data = {'x':x[i,:],'y':y[i]}\n",
    "\n",
    "    return data, problem\n",
    "\n",
    "\n",
    "def pinky(Xin,Yin,dist_in,PLOT,NAME):\n",
    "    # Checking the input.\n",
    "    if len(np.shape(dist_in)) > 2:\n",
    "        print(\"The input must be a N x M matrix.\")\n",
    "        return\n",
    "    sy,sx = np.shape(dist_in)\n",
    "    if (len(Xin) != sx) or (len(Yin) != sy):\n",
    "        print(\"Dimensions of input vectors and input matrix must match.\")\n",
    "        return\n",
    "    for i in range(0,sy):\n",
    "        for j in range(0,sx):\n",
    "            if dist_in[i,j] < 0:\n",
    "                print(\"All input probability values must be positive.\")\n",
    "                return\n",
    "\n",
    "    # Create column distribution. Pick random number.\n",
    "    col_dist = np.sum(dist_in,1)\n",
    "    col_dist /= sum(col_dist)\n",
    "    Xin2 = Xin\n",
    "    Yin2 = Yin\n",
    "\n",
    "    # Generate random value index and saving first value.\n",
    "    ind1 = gendist(col_dist,1,1,PLOT,NAME)\n",
    "    ind1 = np.array(ind1,dtype=\"int\")\n",
    "    x0 = Xin2[ind1]\n",
    "\n",
    "    # Find corresponding indices and weights in the other dimension.\n",
    "    A = (x0-Xin)**2\n",
    "    val_temp = np.sort(A)\n",
    "    ind_temp = np.array([i[0] for i in sorted(enumerate(A), key=lambda x:x[1])])\n",
    "    eps = 2**-52\n",
    "    if val_temp[0] < eps:\n",
    "        row_dist = dist_in[:,ind_temp[0]]\n",
    "    else:\n",
    "        low_val = min(ind_temp[0:2])\n",
    "        high_val = max(ind_temp[0:2])\n",
    "        Xlow = Xin[low_val]\n",
    "        Xhigh = Xin[high_val]\n",
    "        w1 = 1-(x0-Xlow)/(Xhigh-Xlow)\n",
    "        w2 = 1-(Xhigh-x0)/(Xhigh-Xlow)\n",
    "        row_dist = w1*dist_in[:,low_val]+w2*dist_in[:,high_val]\n",
    "    row_dist = row_dist/sum(row_dist)\n",
    "    ind2 = gendist(row_dist,1,1,PLOT,NAME)\n",
    "    y0 = Yin2[ind2]\n",
    "\n",
    "    return x0,y0\n",
    "\n",
    "\n",
    "def gendist(P, N, M, PLOT, NAME):\n",
    "    # Checking input.\n",
    "    if min(P) < 0:\n",
    "        print('All elements of first argument, P, must be positive.')\n",
    "        return\n",
    "    if (N < 1) or (M < 1):\n",
    "        print('Output matrix dimensions must be greater than or equal to one.')\n",
    "        return\n",
    "\n",
    "    # Normalizing P and creating cumlative distribution.\n",
    "    Pnorm = np.concatenate([[0],P],axis=0)/sum(P)\n",
    "    Pcum = np.cumsum(Pnorm)\n",
    "\n",
    "    # Creating random matrix.\n",
    "    R = rnd.rand()\n",
    "\n",
    "    # Calculate output matrix T.\n",
    "    V = np.linspace(0, len(P)-1, len(P))\n",
    "    hist,inds = np.histogram(R, Pcum)\n",
    "    hist = np.argmax(hist)\n",
    "    T = int(V[hist])\n",
    "\n",
    "    # Plotting graphs.\n",
    "    if PLOT == True:\n",
    "        Pfreq = (N*M*P)/sum(P)\n",
    "        LP = len(P)\n",
    "        fig,ax = plt.subplots()\n",
    "        ax.hist(T,np.linspace(1, LP, LP))\n",
    "        ax.plot(Pfreq,color='red')\n",
    "        ax.set_xlabel('Frequency')\n",
    "        ax.set_ylabel('P-vector Index')\n",
    "        fig.savefig(NAME)\n",
    "\n",
    "    return T\n",
    "\n",
    "#if __name__=='__main__':\n",
    "    #data, problem = make_circle_problem(10,50,True)\n",
    "    \n",
    "#############   Make circle problem finished  ##################\n",
    "\n",
    "\n",
    "#############   Our code starts here  ########################\n",
    "    \n",
    "\n",
    "#Defining variables\n",
    "N = 100 #number of points\n",
    "M = 20 #number of steps in Euler method\n",
    "h = 0.03  #stepsize\n",
    "W = np.ones(4) #Projection vector\n",
    "nx = 500 #resolution of data in make circle problem\n",
    "eps = 10E-5 #nummerical differentitation increment\n",
    "tau = 0.5 #Gradient decent parameter\n",
    "TOL = 0.01 #Tolerance\n",
    "\n",
    "\n",
    "#Function to make Y0 with the given data\n",
    "def make_Y0(N, x_values, y_values):\n",
    "    Y0 = np.zeros((N, 4))\n",
    "    for i in range(N):\n",
    "        Y0[i][0] = x_values[i]\n",
    "        Y0[i][1] = y_values[i]\n",
    "        Y0[i][2] = x_values[i]**2\n",
    "        Y0[i][3] = y_values[i]**2\n",
    "\n",
    "    return Y0\n",
    "\n",
    "#Function to get the initial data for Y0 and C on the proper form\n",
    "def Initial_data(N):\n",
    "    #Getting data from make circle problem\n",
    "    data, problem = make_circle_problem(int(N/2), nx, False)\n",
    "    x_values = data['x'][:,0]\n",
    "    y_values = data['x'][:,1]\n",
    "    C_not = data['y']\n",
    "    #making C into a vector and changing the values to 0 and 1 (instead of 1 and 2)\n",
    "    C = np.zeros(len(C_not))\n",
    "    for i in range(0,len(C_not)):\n",
    "        C[i] = C_not[i][0]-1\n",
    "\n",
    "    Y0 = make_Y0(N, x_values, y_values)\n",
    "    \n",
    "    return Y0, C\n",
    "\n",
    "#function to make K0 with M identity matrices\n",
    "def make_K0(M):\n",
    "    K = []\n",
    "    for i in range(M):\n",
    "        K.append(np.identity(4))\n",
    "    return np.array(K)\n",
    "\n",
    "#function to calculate the sigma for the Euler method\n",
    "def sigma(Y):\n",
    "    return np.tanh(Y)\n",
    "\n",
    "# Task 4.1) Function to calculate Euler\n",
    "def Euler(M, h, K, Y0):\n",
    "    Ycurrent = np.copy(Y0)\n",
    "    for i in range(M):\n",
    "        Ynext = Ycurrent + h * sigma(np.matmul(Ycurrent, K[i]))\n",
    "        Ycurrent = Ynext\n",
    "    return Ycurrent\n",
    "\n",
    "#Hypothesis function to make the projection\n",
    "def etha(x):\n",
    "    return np.exp(x) / (np.exp(x) + 1)\n",
    "\n",
    "# Task 4.2) The objective function J\n",
    "def J(W, C, M, h, K, Y0):\n",
    "    YM = Euler(M, h, K, Y0)\n",
    "    norm = (nl.norm(etha(np.matmul(YM,W)) - C, 2))**2\n",
    "    return 0.5*norm  # \n",
    "\n",
    "\n",
    "# Task 4.3) Function for numerical generation of gradient of J\n",
    "def GradCalc(M, h, K, Y0, C, eps, W):\n",
    "    deltaJ = np.zeros((M, 4, 4))\n",
    "    \n",
    "    for m in range(M):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                e = np.zeros((4,4))\n",
    "                e[i,j] = 1\n",
    "                K_tilde = np.matrix.copy(K)\n",
    "                K_tilde[m] += eps*e\n",
    "                J_tilde = J(W, C, M, h, K_tilde, Y0)\n",
    "                deltaJ[m,i,j] = (1/eps)*(J_tilde- J(W, C, M, h, K, Y0))\n",
    "    \n",
    "    deltaJw = np.zeros(4)\n",
    "    for i in range(4):\n",
    "        W_tilde = np.copy(W)\n",
    "        e = np.eye(4)\n",
    "        W_tilde += eps*e[i]\n",
    "        Jw_tilde = J(W_tilde, C, M, h, K, Y0)\n",
    "        deltaJw[i] = (1/eps)*(Jw_tilde- J(W, C, M, h, K, Y0))\n",
    "    \n",
    "    return deltaJ, deltaJw \n",
    "\n",
    "\n",
    "# Task 4.4) The gradient decent algorithm\n",
    "def GDA(h, M, eps, tau, TOL, K, Y0, J, C, W):\n",
    "    J_val = J(W, C, M, h, K, Y0)\n",
    "    i = 0\n",
    "    \n",
    "    while(i < 5000 and J_val > TOL):\n",
    "        i += 1\n",
    "        deltaJ, deltaJw = GradCalc(M, h, K, Y0, C, eps,W)\n",
    "        W = np.subtract(W,tau*deltaJw)\n",
    "        K = np.subtract(K,tau*deltaJ)\n",
    "        J_val = J(W, C, M, h, K, Y0)\n",
    "        #print(J_val)\n",
    "\n",
    "    return J_val, W, K, i \n",
    "\n",
    "# Task 4.5) \n",
    "def main(M, h, N, eta, K, W):\n",
    "    #Initializing data and Running GDA\n",
    "    Y0_1 ,C0_1 = Initial_data(N)\n",
    "    J_val, W, K, i = GDA(h, M, eps, tau, TOL, K, Y0_1, J, C0_1, W)\n",
    "\n",
    "    print(\"J after \", i, \" iterations: \", J_val)\n",
    "    \n",
    "    #Initializing new data for validation\n",
    "    Y0_2, C0_2 = Initial_data(N)\n",
    "    YM2 = Euler(M, h, K, Y0_2)\n",
    "    \n",
    "    ## Validation ##\n",
    "    etha_list = etha(np.matmul(YM2, W)) #calculating projection\n",
    "    \n",
    "    #counting correct cases\n",
    "    numb_correct = 0\n",
    "    for j in range(N):\n",
    "        if (etha_list[j] < 0.5):\n",
    "            if C0_2[j] == 0:\n",
    "                numb_correct += 1\n",
    "        else:\n",
    "            if C0_2[j] == 1:\n",
    "                numb_correct += 1 \n",
    "    \n",
    "    prosent = (numb_correct/N)*100\n",
    "    \n",
    "    print(\"Percentage of cases where points are classified correctly: \", prosent , \"%\")\n",
    "\n",
    "\n",
    "main(M, h, N, etha, make_K0(M), W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7nDfG5Gmcup"
   },
   "source": [
    "With the numerical implementation of the gradient computation, the points were classified correctly on average 98% of the cases.  An analytical implementation would probably have increased the accuracy even more. However, the percentage of correct guesses was judged satisfactorily, and a choice of sticking to the numerical implementation was made. \n",
    "\n",
    "It was also discussed whether to implement an adaptive $h$ or implement a higher order numerical ODE solver, in order to speed up the convergence. Again, the high success rate of the current algorithm is the main motivation for not implementing this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOnJ_h6zr9rC"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXkPYxls1VbK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NumMat proj2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
